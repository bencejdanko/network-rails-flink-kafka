# ./forecast_producer.py
import os
from dotenv import load_dotenv
load_dotenv()

import stomp
import zlib
import io
import time
import socket
import logging
import datetime # Import datetime for type checking - Although not directly used for PyXB times, good practice.

# xml.etree is not strictly needed for pyxb parsing but keeping for context
# import xml.etree.ElementTree as ET

# xmltodict and json are for transformation/output, not parsing
# import xmltodict
import json

from kafka import KafkaProducer
producer = KafkaProducer(bootstrap_servers=f'localhost:{9092}')

logging.basicConfig(format='%(asctime)s %(levelname)s\t%(message)s', level=logging.INFO)

try:
    # Import the top-level binding module generated by PyXB
    import PPv16 as pushport_bindings
    # PyXB usually handles nested imports automatically when you import the top level
    # You generally don't need to explicitly import _for, _ct etc. unless referencing them directly outside parsing.
    # import pyxb_bindings._for # Not strictly needed here if only using via pushport_bindings

except ModuleNotFoundError as e:
    logging.error(f"PyXB binding files not found - please configure the client following steps in README.md! Error: {e}")
    exit(1)
except ImportError as e:
     logging.error(f"Failed to import generated PyXB bindings. Check PYTHONPATH or if bindings exist. Error: {e}")
     exit(1)


USERNAME = os.getenv('USERNAME')
PASSWORD = os.getenv('PASSWORD')
HOSTNAME = os.getenv('HOSTNAME')
HOSTPORT = os.getenv('HOSTPORT')
TOPIC = os.getenv('TOPIC')

KAFKA_TS_TOPIC = 'rtti-ts' # Assuming Kafka is intended later

if not USERNAME or not PASSWORD or not HOSTNAME or not HOSTPORT or not TOPIC:
    logging.error("STOMP connection details (USERNAME, PASSWORD, HOSTNAME, HOSTPORT, TOPIC) not fully set in environment variables!")
    exit(1)

CLIENT_ID = socket.getfqdn()
HEARTBEAT_INTERVAL_MS = 15000
RECONNECT_DELAY_SECS = 15

def transform_ts_message(train_status_obj):
    """
    Transforms a PyXB TrainStatus (TS) object (_for.TS type) into a list
    of flatter dictionaries, one for each location update.
    """
    transformed_station_updates = []
    # Safely get top-level attributes from the TS object (_for.TS)
    rid = getattr(train_status_obj, 'rid', None)
    uid = getattr(train_status_obj, 'uid', None)
    ssd = getattr(train_status_obj, 'ssd', None) # Get scheduled date

    try:
        locations_list = getattr(train_status_obj, 'Location', None)

        if locations_list:
            for location in locations_list: # location is an _for.TSLocation object
                try:
                    tpl = getattr(location, 'tpl', None)

                    # --- CORRECTED TIME CONVERSION ---
                    pta_obj = getattr(location, 'pta', None)
                    # Use str() for PyXB time types
                    pta_str = str(pta_obj) if pta_obj is not None else None

                    ptd_obj = getattr(location, 'ptd', None)
                    # Use str() for PyXB time types
                    ptd_str = str(ptd_obj) if ptd_obj is not None else None

                    wta_obj = getattr(location, 'wta', None)
                    wta_str = str(wta_obj) if wta_obj is not None else None
                    wtd_obj = getattr(location, 'wtd', None)
                    wtd_str = str(wtd_obj) if wtd_obj is not None else None
                    wtp_obj = getattr(location, 'wtp', None)
                    wtp_str = str(wtp_obj) if wtp_obj is not None else None
                    # --- END CORRECTION ---


                    et_str = None
                    at_str = None
                    forecast_source = None
                    is_delayed = False

                    arr_data = getattr(location, 'arr', None)
                    dep_data = getattr(location, 'dep', None)
                    pass_data = getattr(location, 'pass_', None)

                    event_data = None
                    event_type = "none"
                    if arr_data is not None:
                        event_data = arr_data
                        event_type = "arr"
                    elif dep_data is not None:
                        event_data = dep_data
                        event_type = "dep"
                    elif pass_data is not None:
                        event_data = pass_data
                        event_type = "pass"


                    if event_data is not None:
                        # --- CORRECTED TIME CONVERSION ---
                        et_obj = getattr(event_data, 'et', None)
                        # Use str() for PyXB time types
                        et_str = str(et_obj) if et_obj is not None else None

                        at_obj = getattr(event_data, 'at', None)
                        # Use str() for PyXB time types
                        at_str = str(at_obj) if at_obj is not None else None
                        # --- END CORRECTION ---

                        forecast_source = getattr(event_data, 'src', None)
                        is_delayed = getattr(event_data, 'delayed', False)

                    forecast_time_str = at_str if at_str is not None else et_str

                    # --- Combine Date and Time for Full Timestamp (Optional but often useful) ---
                    # Get the date part from ssd (which should be _ct.RTTIDateType -> pyxb.binding.datatypes.date)
                    ssd_date_str = str(ssd) if ssd is not None else None
                    full_pta_ts = f"{ssd_date_str}T{pta_str}" if ssd_date_str and pta_str else None
                    full_ptd_ts = f"{ssd_date_str}T{ptd_str}" if ssd_date_str and ptd_str else None
                    full_forecast_ts = f"{ssd_date_str}T{forecast_time_str}" if ssd_date_str and forecast_time_str else None
                    full_estimated_ts = f"{ssd_date_str}T{et_str}" if ssd_date_str and et_str else None
                    full_actual_ts = f"{ssd_date_str}T{at_str}" if ssd_date_str and at_str else None
                    # Add timezone info if available/needed, e.g., "+01:00" or "Z"
                    # --- End Timestamp Combination ---


                    platform_data = getattr(location, 'plat', None)
                    platform_num = None
                    platform_confirmed = False
                    platform_source = None
                    platform_suppressed = False
                    if platform_data is not None:
                        # Use value() for the content, str() works too for simple types
                        platform_num = str(platform_data.value()) if platform_data.value() is not None else None
                        platform_confirmed = getattr(platform_data, 'conf', False)
                        platform_source = getattr(platform_data, 'platsrc', None)
                        platform_suppressed = getattr(platform_data, 'platsup', False) or getattr(platform_data, 'cisPlatsup', False)

                    transformed_station_updates.append({
                        "rid": rid,
                        "uid": uid,
                        "ssd": ssd_date_str, # Include schedule date
                        # Location specific attributes
                        "tpl": tpl,
                        "pta": pta_str, # Public time arrival (time only)
                        "ptd": ptd_str, # Public time departure (time only)
                        # Forecast specific attributes
                        "event_type": event_type,
                        "forecast_time": forecast_time_str, # Actual/Estimated (time only)
                        "estimated_time": et_str, # Estimated (time only)
                        "actual_time": at_str,     # Actual (time only)
                        # Full Timestamps (Date + Time)
                        "pta_timestamp": full_pta_ts,
                        "ptd_timestamp": full_ptd_ts,
                        "forecast_timestamp": full_forecast_ts,
                        "estimated_timestamp": full_estimated_ts,
                        "actual_timestamp": full_actual_ts,
                        # Flags and Metadata
                        "is_delayed": is_delayed,
                        "forecast_source": forecast_source,
                        # Platform specific attributes
                        "platform": platform_num,
                        "platform_confirmed": platform_confirmed,
                        "platform_source": platform_source,
                        "platform_suppressed": platform_suppressed,
                        # Add other location attributes if needed
                        "wta": wta_str,
                        "wtd": wtd_str,
                        "wtp": wtp_str,
                        # "suppressed_at_loc": getattr(location, 'suppr', False),
                        # "length": getattr(location, 'length', 0),
                        # "detach_front": getattr(location, 'detachFront', False),
                    })

                except AttributeError as ae_loc:
                    logging.error(f"Attribute error processing location for RID {rid}, TPL {getattr(location, 'tpl', 'UNKNOWN')}: {ae_loc}. Check PyXB binding structure.", exc_info=True)
                    continue
                except Exception as e_loc:
                    logging.error(f"Unexpected error processing location for RID {rid}, TPL {getattr(location, 'tpl', 'UNKNOWN')}: {e_loc}", exc_info=True)
                    continue

        return transformed_station_updates

    except AttributeError as ae_ts:
         logging.error(f"Attribute error accessing top-level TS attributes for RID {rid} / UID {uid}: {ae_ts}. Check _for.TS structure.", exc_info=True)
         return []
    except Exception as e:
        logging.error(f"Error transforming TS message for RID {rid}, UID {uid}: {e}", exc_info=True)
        return []

def connect_and_subscribe(connection):
    # Check stomp version and call start() if needed (older versions)
    if hasattr(stomp, '__version__') and isinstance(stomp.__version__, tuple) and stomp.__version__[0] < 5:
         connection.start()
    elif isinstance(stomp.__version__, str) and stomp.__version__.startswith(('1','2','3','4')):
        connection.start()


    connect_header = {'client-id': USERNAME + '-' + CLIENT_ID, 'accept-version': '1.1,1.2'} # Be explicit about supported STOMP versions
    subscribe_header = {'activemq.subscriptionName': CLIENT_ID} # Or another unique sub name

    logging.info(f"Attempting to connect to {HOSTNAME}:{HOSTPORT} with user {USERNAME}")
    try:
        connection.connect(username=USERNAME,
                           passcode=PASSWORD,
                           wait=True,
                           headers=connect_header)
        logging.info("Connection successful.")

        logging.info(f"Subscribing to topic: {TOPIC}")
        connection.subscribe(destination=TOPIC,
                             id='1', # Unique subscription ID for this connection
                             ack='auto', # Messages auto-acknowledged on receipt by client library
                             headers=subscribe_header)
        logging.info("Subscription successful.")
    except stomp.exception.ConnectFailedException as conn_e:
        logging.error(f"STOMP connection failed: {conn_e}")
        raise # Re-raise to be caught in main retry loop
    except Exception as e:
        logging.error(f"Failed to connect or subscribe: {e}", exc_info=True)
        raise # Re-raise


class StompClient(stomp.ConnectionListener):
    def __init__(self, conn):
        self.conn = conn # Store connection internally

    def on_heartbeat(self):
        logging.debug('Received a heartbeat') # Changed to DEBUG
        pass

    def on_heartbeat_timeout(self):
        logging.error('Heartbeat timeout - connection may be lost. Attempting disconnect/reconnect.')
        # Attempt to gracefully disconnect, the main loop's is_connected() check will trigger reconnect
        try:
            if self.conn and self.conn.is_connected():
                self.conn.disconnect()
        except Exception as e:
            logging.warning(f"Exception during disconnect on heartbeat timeout: {e}")
        # The main loop will handle the actual reconnect logic.

    def on_error(self, frame):
        logging.error(f"Received STOMP error frame: {frame.headers} Body: {frame.body}")

    def on_disconnected(self):
        logging.warning('Disconnected from STOMP. Reconnect logic handled by main loop.')
        # The main loop checks conn.is_connected() and will handle the reconnect attempt.

    def on_connecting(self, host_and_port):
        logging.info(f'Attempting STOMP connection to {host_and_port[0]}:{host_and_port[1]}')

    def on_message(self, frame):
        sequence_number = frame.headers.get('SequenceNumber', 'N/A')
        message_type = frame.headers.get('MessageType', 'N/A')
        push_port_sequence = frame.headers.get('PushPortSequence', 'N/A') # Another useful header
        received_time = time.time()
        logging.debug(f'Message sequence={sequence_number}, type={message_type}, pp_seq={push_port_sequence} received')

        try:
            # 1. Decompress
            try:
                msg = zlib.decompress(frame.body, zlib.MAX_WBITS | 32)
                logging.debug(f"Message {sequence_number} successfully decompressed ({len(frame.body)} -> {len(msg)} bytes).")
            except zlib.error as zlib_e:
                 logging.error(f"Zlib decompression error for message sequence={sequence_number}: {zlib_e}. Skipping message.")
                 return # Skip this message

            # 2. Parse XML using PyXB
            try:
                # Expecting the root element to be <Pport> as defined in PPv16.py
                # pushport_bindings.CreateFromDocument should parse the whole structure
                pushport_obj = pushport_bindings.CreateFromDocument(msg)
                logging.debug(f"Message {sequence_number} successfully parsed into PyXB object: {type(pushport_obj)}")
            except Exception as parse_e:
                logging.error(f"PyXB parsing error for message sequence={sequence_number}: {parse_e}", exc_info=True)
                # Optionally log the raw XML if parsing fails frequently for debugging
                # logging.debug(f"Raw XML for failed parse (seq={sequence_number}):\n{msg.decode('utf-8', errors='ignore')[:1000]}...")
                return # Skip this message

            # 3. Process Updates (<uR> container)
            # Access the uR element (Update Response container)
            update_messages_container = getattr(pushport_obj, 'uR', None) # CTD_ANON_4 or None

            if update_messages_container:
                 logging.debug(f"Found uR container in message sequence={sequence_number}")
                 # Access the TS element(s) within uR. This returns a _PluralBinding list.
                 ts_list_obj = getattr(update_messages_container, 'TS', None) # _PluralBinding or None

                 if ts_list_obj:
                     # Check if the list is non-empty before logging count
                     if len(ts_list_obj) > 0:
                        logging.info(f"Found {len(ts_list_obj)} Train Status (TS) updates in message sequence={sequence_number}.")

                        for train_status_obj in ts_list_obj: # Iterate through individual _for.TS objects
                            ts_rid = getattr(train_status_obj, 'rid', 'UNKNOWN_RID') # Get RID safely for logging
                            ts_uid = getattr(train_status_obj, 'uid', 'UNKNOWN_UID') # Get UID safely for logging
                            try:
                                # Transform the individual TS object (_for.TS)
                                transformed_station_data_list = transform_ts_message(train_status_obj)

                                if transformed_station_data_list:
                                    logging.debug(f"Transformed {len(transformed_station_data_list)} locations for TS RID={ts_rid}.")
                                    # Process each transformed station update dictionary
                                    for station_data in transformed_station_data_list:
                                        # Add processing timestamp
                                        station_data['processing_ts'] = received_time
                                        station_data['pp_sequence'] = push_port_sequence

                                        # Optional: Filter out updates with no forecast time or TPL?
                                        # if station_data.get('tpl') is None or station_data.get('forecast_time') is None:
                                        #     logging.debug(f"Skipping update for RID {ts_rid} due to missing TPL or forecast time.")
                                        #     continue

                                        # Log/print the transformed data
                                        logging.debug(f"Transformed TS Loc Update: {json.dumps(station_data)}")

                                        # Prepare for Kafka (or logging to file)
                                        json_payload = json.dumps(station_data).encode('utf-8')

                                        # Example: Append to file
                                        try:
                                            with open("forecast_updates.log", "a") as f:
                                                f.write(json.dumps(station_data) + "\n")
                                            logging.debug(f"Logged forecast update for RID {ts_rid} TPL {station_data.get('tpl')} to forecast_updates.log")
                                        except IOError as e:
                                            logging.error(f"Could not write to forecast_updates.log: {e}")

                                        # Example: Send to Kafka (uncomment if using Kafka)
                                        try:
                                            # Use TPL or RID as key? RID might be better for partitioning train journeys.
                                            key = str(ts_rid).encode('utf-8')
                                            producer.send(KAFKA_TS_TOPIC, key=key, value=json_payload)
                                            logging.debug(f"Produced TS station message for RID {ts_rid} TPL {station_data.get('tpl')} to Kafka topic {KAFKA_TS_TOPIC}")
                                        except Exception as kafka_e:
                                            logging.error(f"Failed to send message to Kafka: {kafka_e}")

                                else:
                                     # Use getattr for safety, although train_status_obj should be valid here
                                     logging.warning(f"TS message transformation completed for RID {ts_rid} but no valid station updates were generated.")

                            except Exception as process_ts_e:
                                # Catch errors during the transformation or processing of a single TS object
                                logging.error(f"Error processing individual TS message (RID={ts_rid}, UID={ts_uid}): {process_ts_e}", exc_info=True)
                                # Continue to the next TS object in the list

                     else:
                         logging.debug(f"uR container found in message {sequence_number}, but it contains no TS elements.")

                 # Handle other message types within the uR container if necessary
                 elif getattr(update_messages_container, 'schedule', None): # Example check for schedule
                     logging.debug(f"Found schedule update in uR (seq={sequence_number}). Ignoring.")
                 elif getattr(update_messages_container, 'association', None): # Example check for association
                      logging.debug(f"Found association update in uR (seq={sequence_number}). Ignoring.")
                 # Add checks for OW, trainAlert, trainOrder, etc. if needed
                 else:
                      logging.debug(f"uR container in message {sequence_number} did not contain TS or other handled types.")


            # 4. Handle Snapshot Response (<sR> container) - similar structure to <uR>
            elif getattr(pushport_obj, 'sR', None):
                logging.info(f"Received Snapshot Response (sR) message sequence={sequence_number}. Processing TS within sR.")
                snapshot_container = getattr(pushport_obj, 'sR')
                ts_list_obj_sr = getattr(snapshot_container, 'TS', None)
                if ts_list_obj_sr:
                    if len(ts_list_obj_sr) > 0:
                        logging.info(f"Found {len(ts_list_obj_sr)} Train Status (TS) updates in sR message sequence={sequence_number}.")
                        # Identical processing logic as for uR... could refactor into a helper function
                        for train_status_obj in ts_list_obj_sr:
                            ts_rid = getattr(train_status_obj, 'rid', 'UNKNOWN_RID')
                            ts_uid = getattr(train_status_obj, 'uid', 'UNKNOWN_UID')
                            try:
                                transformed_station_data_list = transform_ts_message(train_status_obj)
                                if transformed_station_data_list:
                                    logging.debug(f"Transformed {len(transformed_station_data_list)} locations for TS RID={ts_rid} from sR.")
                                    for station_data in transformed_station_data_list:
                                        station_data['processing_ts'] = received_time
                                        station_data['pp_sequence'] = push_port_sequence
                                        logging.debug(f"Transformed TS Loc Update from sR: {json.dumps(station_data)}")
                                        json_payload = json.dumps(station_data).encode('utf-8')
                                        try:
                                            with open("forecast_updates_snapshot.log", "a") as f: # Log snapshots separately?
                                                f.write(json.dumps(station_data) + "\n")
                                            logging.debug(f"Logged snapshot forecast update for RID {ts_rid} TPL {station_data.get('tpl')} to forecast_updates_snapshot.log")
                                        except IOError as e:
                                            logging.error(f"Could not write to forecast_updates_snapshot.log: {e}")
                                        # Send to Kafka if needed
                                else:
                                    logging.warning(f"TS message (from sR) transformation completed for RID {ts_rid} but no valid station updates generated.")
                            except Exception as process_ts_e:
                                logging.error(f"Error processing individual TS message from sR (RID={ts_rid}, UID={ts_uid}): {process_ts_e}", exc_info=True)
                    else:
                        logging.debug(f"sR container found in message {sequence_number}, but it contains no TS elements.")
                else:
                     logging.debug(f"sR container in message {sequence_number} did not contain TS elements.")


            # 5. Handle other top-level message types if needed (e.g., FailureResp)
            elif getattr(pushport_obj, 'FailureResp', None):
                failure_resp = getattr(pushport_obj, 'FailureResp')
                logging.error(f"Received Failure Response message sequence={sequence_number}: Code={getattr(failure_resp,'code','N/A')} Body={failure_resp.value()}")

            # 6. Fallback for unhandled structures
            else:
                # Check MessageType header again for clues if parsing succeeded but no known container was found
                if message_type == 'TS':
                     logging.warning(f"Message {sequence_number} header Type=TS, but no uR or sR container found in parsed object {type(pushport_obj)}. Structure might be unexpected.")
                elif message_type in ['SC', 'AS', 'OW', 'TA', 'TO', 'TD', 'LA']: # Add other known types
                     logging.info(f"Ignoring message type '{message_type}' (Sequence={sequence_number}) as per current logic.")
                else:
                     logging.warning(f"Received unhandled message type '{message_type}' or unknown structure in parsed object (Sequence={sequence_number}, Type={type(pushport_obj)}).")


        except Exception as general_e:
            # Catch-all for unexpected errors during message processing after parsing
            logging.error(f"General error processing message sequence={sequence_number}, type={message_type}: {general_e}", exc_info=True)
            # If ack='client', you might want to frame.nack() here for failed messages


if __name__ == "__main__":
    conn = None
    while True:
        try:
            logging.info("Attempting to establish STOMP connection...")
            # Use Connection12 explicitly if required by broker, otherwise Connection defaults usually work
            conn = stomp.Connection12([(HOSTNAME, HOSTPORT)],
                                      auto_decode=False, # We handle decoding after decompression
                                      heartbeats=(HEARTBEAT_INTERVAL_MS, HEARTBEAT_INTERVAL_MS))

            # Pass the connection object to the listener upon creation
            listener = StompClient(conn)
            conn.set_listener('', listener)

            connect_and_subscribe(conn) # Handles connect and subscribe logic

            logging.info("Client connected and subscribed. Listening for messages...")

            # Keep the main thread alive while the listener handles messages in its own thread (stomp.py internals)
            while conn.is_connected():
                time.sleep(1) # Check connection status periodically

            # If loop exits, it means is_connected() returned False (likely due to on_disconnected or on_heartbeat_timeout)
            logging.warning(f"Connection lost detected by main loop. Attempting to reconnect in {RECONNECT_DELAY_SECS} seconds...")
            # Clean disconnect just in case listener didn't fully disconnect
            try:
                if conn and not conn.transport.is_connected(): # Check transport layer specifically
                     pass # Already disconnected
                elif conn:
                     conn.disconnect()
            except Exception as disc_e:
                 logging.warning(f"Exception during cleanup disconnect: {disc_e}")
            conn = None # Ensure we create a new connection object next iteration
            time.sleep(RECONNECT_DELAY_SECS)

        except (stomp.exception.ConnectFailedException, stomp.exception.NotConnectedException) as stomp_e:
             logging.error(f"STOMP Connection Error: {stomp_e}. Retrying in {RECONNECT_DELAY_SECS} seconds...")
             if conn:
                 try:
                     conn.disconnect()
                 except Exception as disc_e:
                     logging.warning(f"Exception during disconnect after STOMP error: {disc_e}")
             conn = None
             time.sleep(RECONNECT_DELAY_SECS)
        except KeyboardInterrupt:
             logging.info("KeyboardInterrupt received. Shutting down...")
             break # Exit the main loop
        except Exception as e:
            # Catch other potential errors during connection setup/main loop
            logging.error(f"Unexpected error in main loop: {e}", exc_info=True)
            if conn and conn.is_connected():
                 try:
                     conn.disconnect()
                 except Exception as disc_e:
                     logging.error(f"Error during disconnect after unexpected error: {disc_e}")
            conn = None
            logging.warning(f"Retrying connection in {RECONNECT_DELAY_SECS} seconds...")
            time.sleep(RECONNECT_DELAY_SECS)
        # No finally block needed here as the loop handles cleanup/retry

    # Graceful shutdown
    logging.info("Client shutdown sequence initiated.")
    if conn and conn.is_connected():
        try:
            logging.info("Disconnecting active STOMP connection...")
            conn.disconnect()
            logging.info("STOMP connection disconnected.")
        except Exception as disc_e:
             logging.error(f"Error during final disconnect: {disc_e}")

    # Kafka producer shutdown (uncomment if using Kafka)
    # logging.info("Closing Kafka producer...")
    # if 'producer' in locals() and producer:
    #     producer.flush() # Ensure all messages are sent
    #     producer.close()
    #     logging.info("Kafka producer closed.")

    logging.info("Client has shut down.")