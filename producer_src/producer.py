# ./forecast_producer.py
import os
from dotenv import load_dotenv
load_dotenv()

import stomp
import zlib
import io
import time
import socket
import logging
import datetime # Import datetime for type checking - Although not directly used for PyXB times, good practice.

# xml.etree is not strictly needed for pyxb parsing but keeping for context
# import xml.etree.ElementTree as ET

# xmltodict and json are for transformation/output, not parsing
# import xmltodict
import json

from kafka import KafkaProducer
producer = KafkaProducer(bootstrap_servers=f'localhost:{9092}')

logging.basicConfig(format='%(asctime)s %(levelname)s\t%(message)s', level=logging.INFO)

try:
    # Import the top-level binding module generated by PyXB
    import PPv16 as pushport_bindings
    # PyXB usually handles nested imports automatically when you import the top level
    # You generally don't need to explicitly import _for, _ct etc. unless referencing them directly outside parsing.
    # import pyxb_bindings._for # Not strictly needed here if only using via pushport_bindings

except ModuleNotFoundError as e:
    logging.error(f"PyXB binding files not found - please configure the client following steps in README.md! Error: {e}")
    exit(1)
except ImportError as e:
     logging.error(f"Failed to import generated PyXB bindings. Check PYTHONPATH or if bindings exist. Error: {e}")
     exit(1)


USERNAME = os.getenv('USERNAME')
PASSWORD = os.getenv('PASSWORD')
HOSTNAME = os.getenv('HOSTNAME')
HOSTPORT = os.getenv('HOSTPORT')
TOPIC = os.getenv('TOPIC')

KAFKA_TS_TOPIC = 'rtti-ts' # Assuming Kafka is intended later
KAFKA_TS_TOPIC = 'rtti-ts'

if not USERNAME or not PASSWORD or not HOSTNAME or not HOSTPORT or not TOPIC:
    logging.error("STOMP connection details (USERNAME, PASSWORD, HOSTNAME, HOSTPORT, TOPIC) not fully set in environment variables!")
    exit(1)

CLIENT_ID = socket.getfqdn()
HEARTBEAT_INTERVAL_MS = 15000
RECONNECT_DELAY_SECS = 15

def transform_schedule_message(schedule_obj):
    """
    Transforms a PyXB Schedule object (_sch3.Schedule type) into a list
    of flatter dictionaries, one for each scheduled calling point/location.
    """
    transformed_schedule_locations = []

    # --- Extract top-level Schedule attributes ---
    rid = getattr(schedule_obj, 'rid', None)
    uid = getattr(schedule_obj, 'uid', None)
    trainId = getattr(schedule_obj, 'trainId', None)
    rsid = getattr(schedule_obj, 'rsid', None)
    ssd_obj = getattr(schedule_obj, 'ssd', None) # _ct.RTTIDateType
    ssd_str = str(ssd_obj) if ssd_obj is not None else None
    toc = getattr(schedule_obj, 'toc', None)
    status = getattr(schedule_obj, 'status', 'P') # Default 'P'
    trainCat = getattr(schedule_obj, 'trainCat', 'OO') # Default 'OO'
    isPassengerSvc = getattr(schedule_obj, 'isPassengerSvc', True) # Default True
    isActive = getattr(schedule_obj, 'isActive', True) # Default True
    deleted = getattr(schedule_obj, 'deleted', False) # Default False
    isCharter = getattr(schedule_obj, 'isCharter', False) # Default False

    # Extract cancellation reason if present (_ct.DisruptionReasonType)
    cancel_reason_obj = getattr(schedule_obj, 'cancelReason', None)
    cancel_reason_code = None
    cancel_reason_near = None
    cancel_reason_tiploc = None
    if cancel_reason_obj is not None:
        cancel_reason_code = cancel_reason_obj.value() # The code is the simple content
        cancel_reason_near = getattr(cancel_reason_obj, 'near', None)
        cancel_reason_tiploc = getattr(cancel_reason_obj, 'tip', None) # Attribute name is 'tip'

    # --- Define a helper to process common location attributes ---
    def process_location(loc_obj, loc_type):
        tpl = getattr(loc_obj, 'tpl', None)
        act = getattr(loc_obj, 'act', '  ') # Default '  '
        planAct = getattr(loc_obj, 'planAct', None)
        can = getattr(loc_obj, 'can', False) # Default False
        fid = getattr(loc_obj, 'fid', None)
        avgLoading = getattr(loc_obj, 'avgLoading', None)
        rdelay_obj = getattr(loc_obj, 'rdelay', 0) # Default 0 (check type, might be string '0')
        # Ensure rdelay is integer or None
        try:
            rdelay = int(rdelay_obj) if rdelay_obj is not None else 0
        except (ValueError, TypeError):
             rdelay = 0 # Default if conversion fails
        fd = getattr(loc_obj, 'fd', None)

        # Extract and format times using str()
        pta_obj = getattr(loc_obj, 'pta', None)
        pta_str = str(pta_obj) if pta_obj is not None else None
        ptd_obj = getattr(loc_obj, 'ptd', None)
        ptd_str = str(ptd_obj) if ptd_obj is not None else None
        wta_obj = getattr(loc_obj, 'wta', None)
        wta_str = str(wta_obj) if wta_obj is not None else None
        wtd_obj = getattr(loc_obj, 'wtd', None)
        wtd_str = str(wtd_obj) if wtd_obj is not None else None
        wtp_obj = getattr(loc_obj, 'wtp', None)
        wtp_str = str(wtp_obj) if wtp_obj is not None else None

        # Combine Date and Time for Full Timestamps
        full_pta_ts = f"{ssd_str}T{pta_str}" if ssd_str and pta_str else None
        full_ptd_ts = f"{ssd_str}T{ptd_str}" if ssd_str and ptd_str else None
        full_wta_ts = f"{ssd_str}T{wta_str}" if ssd_str and wta_str else None
        full_wtd_ts = f"{ssd_str}T{wtd_str}" if ssd_str and wtd_str else None
        full_wtp_ts = f"{ssd_str}T{wtp_str}" if ssd_str and wtp_str else None

        return {
            # Schedule Header Info (repeated for each location)
            "rid": rid,
            "uid": uid,
            "trainId": trainId,
            "rsid": rsid,
            "ssd": ssd_str,
            "toc": toc,
            "status": status,
            "trainCat": trainCat,
            "isPassengerSvc": isPassengerSvc,
            "isActive": isActive,
            "isDeleted": deleted,
            "isCharter": isCharter,
            "cancelReasonCode": cancel_reason_code,
            "cancelReasonNear": cancel_reason_near,
            "cancelReasonTiploc": cancel_reason_tiploc,
            # Location specific details
            "location_type": loc_type,
            "tpl": tpl,
            "activity": act,
            "planned_activity": planAct,
            "cancelled_at_loc": can,
            "formation_id": fid,
            "avg_loading": avgLoading,
            "route_delay": rdelay,
            "false_destination": fd,
            # Scheduled times (time only strings)
            "pta": pta_str,
            "ptd": ptd_str,
            "wta": wta_str,
            "wtd": wtd_str,
            "wtp": wtp_str,
            # Full Timestamps (Date + Time)
            "pta_timestamp": full_pta_ts,
            "ptd_timestamp": full_ptd_ts,
            "wta_timestamp": full_wta_ts,
            "wtd_timestamp": full_wtd_ts,
            "wtp_timestamp": full_wtp_ts,
        }

    # --- Iterate through all location types ---
    # PyXB creates _PluralBinding lists even for single occurrences
    location_types = {
        "OR": getattr(schedule_obj, 'OR', []),
        "OPOR": getattr(schedule_obj, 'OPOR', []),
        "IP": getattr(schedule_obj, 'IP', []),
        "OPIP": getattr(schedule_obj, 'OPIP', []),
        "PP": getattr(schedule_obj, 'PP', []),
        "DT": getattr(schedule_obj, 'DT', []),
        "OPDT": getattr(schedule_obj, 'OPDT', [])
    }

    try:
        for loc_type, locations in location_types.items():
            if locations: # Check if the list is not empty
                for loc_obj in locations: # Iterate through the _PluralBinding list
                    try:
                         processed_data = process_location(loc_obj, loc_type)
                         if processed_data.get("tpl"): # Basic check for validity
                             transformed_schedule_locations.append(processed_data)
                         else:
                              logging.warning(f"Skipping schedule location of type {loc_type} for RID {rid} due to missing TPL.")
                    except Exception as e_loc:
                         # Error processing a single location within the schedule
                         logging.error(f"Error processing schedule location type {loc_type}, TPL {getattr(loc_obj, 'tpl', 'UNKNOWN')} for RID {rid}: {e_loc}", exc_info=True)
                         continue # Skip to next location

        # Sort by working time (WTA/WTD/WTP) if needed? Schedules are usually ordered.
        # Example sorting (might need refinement based on which time is primary)
        # def get_sort_key(item):
        #     ts = item.get('wta_timestamp') or item.get('wtd_timestamp') or item.get('wtp_timestamp')
        #     return ts if ts else '9999-99-99T99:99:99' # Put items without time at the end
        # transformed_schedule_locations.sort(key=get_sort_key)

        return transformed_schedule_locations

    except Exception as e:
        logging.error(f"General error transforming Schedule message for RID {rid}, UID {uid}: {e}", exc_info=True)
        return [] # Return empty list on failure

def transform_ts_message(train_status_obj):
    """
    Transforms a PyXB TrainStatus (TS) object (_for.TS type) into a list
    of flatter dictionaries, one for each location update.
    """
    transformed_station_updates = []
    # Safely get top-level attributes from the TS object (_for.TS)
    rid = getattr(train_status_obj, 'rid', None)
    uid = getattr(train_status_obj, 'uid', None)
    ssd = getattr(train_status_obj, 'ssd', None) # Get scheduled date

    try:
        locations_list = getattr(train_status_obj, 'Location', None)

        if locations_list:
            for location in locations_list: # location is an _for.TSLocation object
                try:
                    tpl = getattr(location, 'tpl', None)

                    # --- CORRECTED TIME CONVERSION ---
                    pta_obj = getattr(location, 'pta', None)
                    # Use str() for PyXB time types
                    pta_str = str(pta_obj) if pta_obj is not None else None

                    ptd_obj = getattr(location, 'ptd', None)
                    # Use str() for PyXB time types
                    ptd_str = str(ptd_obj) if ptd_obj is not None else None

                    wta_obj = getattr(location, 'wta', None)
                    wta_str = str(wta_obj) if wta_obj is not None else None
                    wtd_obj = getattr(location, 'wtd', None)
                    wtd_str = str(wtd_obj) if wtd_obj is not None else None
                    wtp_obj = getattr(location, 'wtp', None)
                    wtp_str = str(wtp_obj) if wtp_obj is not None else None
                    # --- END CORRECTION ---


                    et_str = None
                    at_str = None
                    forecast_source = None
                    is_delayed = False

                    arr_data = getattr(location, 'arr', None)
                    dep_data = getattr(location, 'dep', None)
                    pass_data = getattr(location, 'pass_', None)

                    event_data = None
                    event_type = "none"
                    if arr_data is not None:
                        event_data = arr_data
                        event_type = "arr"
                    elif dep_data is not None:
                        event_data = dep_data
                        event_type = "dep"
                    elif pass_data is not None:
                        event_data = pass_data
                        event_type = "pass"


                    if event_data is not None:
                        # --- CORRECTED TIME CONVERSION ---
                        et_obj = getattr(event_data, 'et', None)
                        # Use str() for PyXB time types
                        et_str = str(et_obj) if et_obj is not None else None

                        at_obj = getattr(event_data, 'at', None)
                        # Use str() for PyXB time types
                        at_str = str(at_obj) if at_obj is not None else None
                        # --- END CORRECTION ---

                        forecast_source = getattr(event_data, 'src', None)
                        is_delayed = getattr(event_data, 'delayed', False)

                    forecast_time_str = at_str if at_str is not None else et_str

                    # --- Combine Date and Time for Full Timestamp (Optional but often useful) ---
                    # Get the date part from ssd (which should be _ct.RTTIDateType -> pyxb.binding.datatypes.date)
                    ssd_date_str = str(ssd) if ssd is not None else None
                    full_pta_ts = f"{ssd_date_str}T{pta_str}" if ssd_date_str and pta_str else None
                    full_ptd_ts = f"{ssd_date_str}T{ptd_str}" if ssd_date_str and ptd_str else None
                    full_forecast_ts = f"{ssd_date_str}T{forecast_time_str}" if ssd_date_str and forecast_time_str else None
                    full_estimated_ts = f"{ssd_date_str}T{et_str}" if ssd_date_str and et_str else None
                    full_actual_ts = f"{ssd_date_str}T{at_str}" if ssd_date_str and at_str else None
                    # Add timezone info if available/needed, e.g., "+01:00" or "Z"
                    # --- End Timestamp Combination ---


                    platform_data = getattr(location, 'plat', None)
                    platform_num = None
                    platform_confirmed = False
                    platform_source = None
                    platform_suppressed = False
                    if platform_data is not None:
                        # Use value() for the content, str() works too for simple types
                        platform_num = str(platform_data.value()) if platform_data.value() is not None else None
                        platform_confirmed = getattr(platform_data, 'conf', False)
                        platform_source = getattr(platform_data, 'platsrc', None)
                        platform_suppressed = getattr(platform_data, 'platsup', False) or getattr(platform_data, 'cisPlatsup', False)

                    transformed_station_updates.append({
                        "rid": rid,
                        "uid": uid,
                        "ssd": ssd_date_str, # Include schedule date
                        # Location specific attributes
                        "tpl": tpl,
                        "pta": pta_str, # Public time arrival (time only)
                        "ptd": ptd_str, # Public time departure (time only)
                        # Forecast specific attributes
                        "event_type": event_type,
                        "forecast_time": forecast_time_str, # Actual/Estimated (time only)
                        "estimated_time": et_str, # Estimated (time only)
                        "actual_time": at_str,     # Actual (time only)
                        # Full Timestamps (Date + Time)
                        "pta_timestamp": full_pta_ts,
                        "ptd_timestamp": full_ptd_ts,
                        "forecast_timestamp": full_forecast_ts,
                        "estimated_timestamp": full_estimated_ts,
                        "actual_timestamp": full_actual_ts,
                        # Flags and Metadata
                        "is_delayed": is_delayed,
                        "forecast_source": forecast_source,
                        # Platform specific attributes
                        "platform": platform_num,
                        "platform_confirmed": platform_confirmed,
                        "platform_source": platform_source,
                        "platform_suppressed": platform_suppressed,
                        # Add other location attributes if needed
                        "wta": wta_str,
                        "wtd": wtd_str,
                        "wtp": wtp_str,
                        # "suppressed_at_loc": getattr(location, 'suppr', False),
                        # "length": getattr(location, 'length', 0),
                        # "detach_front": getattr(location, 'detachFront', False),
                    })

                except AttributeError as ae_loc:
                    logging.error(f"Attribute error processing location for RID {rid}, TPL {getattr(location, 'tpl', 'UNKNOWN')}: {ae_loc}. Check PyXB binding structure.", exc_info=True)
                    continue
                except Exception as e_loc:
                    logging.error(f"Unexpected error processing location for RID {rid}, TPL {getattr(location, 'tpl', 'UNKNOWN')}: {e_loc}", exc_info=True)
                    continue

        return transformed_station_updates

    except AttributeError as ae_ts:
         logging.error(f"Attribute error accessing top-level TS attributes for RID {rid} / UID {uid}: {ae_ts}. Check _for.TS structure.", exc_info=True)
         return []
    except Exception as e:
        logging.error(f"Error transforming TS message for RID {rid}, UID {uid}: {e}", exc_info=True)
        return []

def connect_and_subscribe(connection):
    # Check stomp version and call start() if needed (older versions)
    if hasattr(stomp, '__version__') and isinstance(stomp.__version__, tuple) and stomp.__version__[0] < 5:
         connection.start()
    elif isinstance(stomp.__version__, str) and stomp.__version__.startswith(('1','2','3','4')):
        connection.start()


    connect_header = {'client-id': USERNAME + '-' + CLIENT_ID, 'accept-version': '1.1,1.2'} # Be explicit about supported STOMP versions
    subscribe_header = {'activemq.subscriptionName': CLIENT_ID} # Or another unique sub name

    logging.info(f"Attempting to connect to {HOSTNAME}:{HOSTPORT} with user {USERNAME}")
    try:
        connection.connect(username=USERNAME,
                           passcode=PASSWORD,
                           wait=True,
                           headers=connect_header)
        logging.info("Connection successful.")

        logging.info(f"Subscribing to topic: {TOPIC}")
        connection.subscribe(destination=TOPIC,
                             id='1', # Unique subscription ID for this connection
                             ack='auto', # Messages auto-acknowledged on receipt by client library
                             headers=subscribe_header)
        logging.info("Subscription successful.")
    except stomp.exception.ConnectFailedException as conn_e:
        logging.error(f"STOMP connection failed: {conn_e}")
        raise # Re-raise to be caught in main retry loop
    except Exception as e:
        logging.error(f"Failed to connect or subscribe: {e}", exc_info=True)
        raise # Re-raise


class StompClient(stomp.ConnectionListener):
    def __init__(self, conn):
        self.conn = conn # Store connection internally

    def on_heartbeat(self):
        logging.debug('Received a heartbeat') # Changed to DEBUG
        pass

    def on_heartbeat_timeout(self):
        logging.error('Heartbeat timeout - connection may be lost. Attempting disconnect/reconnect.')
        # Attempt to gracefully disconnect, the main loop's is_connected() check will trigger reconnect
        try:
            if self.conn and self.conn.is_connected():
                self.conn.disconnect()
        except Exception as e:
            logging.warning(f"Exception during disconnect on heartbeat timeout: {e}")
        # The main loop will handle the actual reconnect logic.

    def on_error(self, frame):
        logging.error(f"Received STOMP error frame: {frame.headers} Body: {frame.body}")

    def on_disconnected(self):
        logging.warning('Disconnected from STOMP. Reconnect logic handled by main loop.')
        # The main loop checks conn.is_connected() and will handle the reconnect attempt.

    def on_connecting(self, host_and_port):
        logging.info(f'Attempting STOMP connection to {host_and_port[0]}:{host_and_port[1]}')

    def on_message(self, frame):
        sequence_number = frame.headers.get('SequenceNumber', 'N/A')
        message_type = frame.headers.get('MessageType', 'N/A')
        push_port_sequence = frame.headers.get('PushPortSequence', 'N/A')
        received_time = time.time()
        logging.debug(f'Message sequence={sequence_number}, type={message_type}, pp_seq={push_port_sequence} received')

        try:
            # 1. Decompress
            try:
                msg = zlib.decompress(frame.body, zlib.MAX_WBITS | 32)
                logging.debug(f"Message {sequence_number} successfully decompressed ({len(frame.body)} -> {len(msg)} bytes).")
            except zlib.error as zlib_e:
                 logging.error(f"Zlib decompression error for message sequence={sequence_number}: {zlib_e}. Skipping message.")
                 return

            # 2. Parse XML
            try:
                pushport_obj = pushport_bindings.CreateFromDocument(msg)
                logging.debug(f"Message {sequence_number} successfully parsed into PyXB object: {type(pushport_obj)}")
            except Exception as parse_e:
                logging.error(f"PyXB parsing error for message sequence={sequence_number}: {parse_e}", exc_info=True)
                return

            # 3. Process Updates (<uR> container)
            update_messages_container = getattr(pushport_obj, 'uR', None)
            if update_messages_container:
                 logging.debug(f"Found uR container in message sequence={sequence_number}")
                 # Check for Train Status (TS)
                 ts_list_obj = getattr(update_messages_container, 'TS', None)
                 # Check for Schedule (schedule) - Note: element name is 'schedule' in DataResponse
                 schedule_list_obj = getattr(update_messages_container, 'schedule', None)

                 if ts_list_obj:
                    # Check if the list is non-empty before logging count
                    if len(ts_list_obj) > 0:
                        logging.info(f"Found {len(ts_list_obj)} Train Status (TS) updates in message sequence={sequence_number}.")

                    for train_status_obj in ts_list_obj: # Iterate through individual _for.TS objects
                        ts_rid = getattr(train_status_obj, 'rid', 'UNKNOWN_RID') # Get RID safely for logging
                        ts_uid = getattr(train_status_obj, 'uid', 'UNKNOWN_UID') # Get UID safely for logging
                        try:
                            # Transform the individual TS object (_for.TS)
                            transformed_station_data_list = transform_ts_message(train_status_obj)

                            if transformed_station_data_list:
                                logging.debug(f"Transformed {len(transformed_station_data_list)} locations for TS RID={ts_rid}.")
                                # Process each transformed station update dictionary
                                for station_data in transformed_station_data_list:
                                    # Add processing timestamp
                                    station_data['processing_ts'] = received_time
                                    station_data['pp_sequence'] = push_port_sequence

                                    # Optional: Filter out updates with no forecast time or TPL?
                                    # if station_data.get('tpl') is None or station_data.get('forecast_time') is None:
                                    #     logging.debug(f"Skipping update for RID {ts_rid} due to missing TPL or forecast time.")
                                    #     continue

                                    # Log/print the transformed data
                                    logging.debug(f"Transformed TS Loc Update: {json.dumps(station_data)}")

                                    # Prepare for Kafka (or logging to file)
                                    json_payload = json.dumps(station_data).encode('utf-8')

                                    # Example: Append to file
                                    try:
                                        with open("forecast_updates.log", "a") as f:
                                            f.write(json.dumps(station_data) + "\n")
                                        logging.debug(f"Logged forecast update for RID {ts_rid} TPL {station_data.get('tpl')} to forecast_updates.log")
                                    except IOError as e:
                                        logging.error(f"Could not write to forecast_updates.log: {e}")

                                    # Example: Send to Kafka (uncomment if using Kafka)
                                    try:
                                        # Use TPL or RID as key? RID might be better for partitioning train journeys.
                                        key = str(ts_rid).encode('utf-8')
                                        producer.send(KAFKA_TS_TOPIC, key=key, value=json_payload)
                                        logging.debug(f"Produced TS station message for RID {ts_rid} TPL {station_data.get('tpl')} to Kafka topic {KAFKA_TS_TOPIC}")
                                    except Exception as kafka_e:
                                        logging.error(f"Failed to send message to Kafka: {kafka_e}")

                            else:
                                    # Use getattr for safety, although train_status_obj should be valid here
                                    logging.warning(f"TS message transformation completed for RID {ts_rid} but no valid station updates were generated.")

                        except Exception as process_ts_e:
                            # Catch errors during the transformation or processing of a single TS object
                            logging.error(f"Error processing individual TS message (RID={ts_rid}, UID={ts_uid}): {process_ts_e}", exc_info=True)
                            # Continue to the next TS object in the list

                    else:
                        logging.debug(f"uR container found in message {sequence_number}, but it contains no TS elements.")

                 # *** NEW: Handle Schedule messages ***
                 elif schedule_list_obj:
                     if len(schedule_list_obj) > 0:
                        logging.info(f"Found {len(schedule_list_obj)} Schedule (schedule) updates in uR message sequence={sequence_number}.")
                        for schedule_obj in schedule_list_obj: # schedule_obj is _sch3.Schedule
                            sc_rid = getattr(schedule_obj, 'rid', 'UNKNOWN_RID')
                            sc_uid = getattr(schedule_obj, 'uid', 'UNKNOWN_UID')
                            try:
                                # Transform the individual Schedule object (_sch3.Schedule)
                                transformed_schedule_locs = transform_schedule_message(schedule_obj)

                                if transformed_schedule_locs:
                                    logging.debug(f"Transformed {len(transformed_schedule_locs)} locations for Schedule RID={sc_rid}.")
                                    # Process each transformed schedule location dictionary
                                    for schedule_loc_data in transformed_schedule_locs:
                                        # Add processing timestamp
                                        schedule_loc_data['processing_ts'] = received_time
                                        schedule_loc_data['pp_sequence'] = push_port_sequence

                                        logging.debug(f"Transformed Schedule Loc Update: {json.dumps(schedule_loc_data)}")

                                        json_payload = json.dumps(schedule_loc_data).encode('utf-8')

                                        # Example: Append to a separate schedule log file
                                        try:
                                            with open("schedule_updates.log", "a") as f:
                                                f.write(json.dumps(schedule_loc_data) + "\n")
                                            logging.debug(f"Logged schedule update for RID {sc_rid} TPL {schedule_loc_data.get('tpl')} to schedule_updates.log")
                                        except IOError as e:
                                            logging.error(f"Could not write to schedule_updates.log: {e}")

                                        KAFKA_SCHEDULE_TOPIC = 'rtti-schedule'
                                        try:
                                            key = str(sc_rid).encode('utf-8')
                                            producer.send(KAFKA_SCHEDULE_TOPIC, key=key, value=json_payload)
                                            logging.debug(f"Produced Schedule location message for RID {sc_rid} TPL {schedule_loc_data.get('tpl')} to Kafka topic {KAFKA_SCHEDULE_TOPIC}")
                                        except Exception as kafka_e:
                                            logging.error(f"Failed to send schedule message to Kafka: {kafka_e}")

                                else:
                                     logging.warning(f"Schedule message transformation completed for RID {sc_rid} but no valid locations were generated.")

                            except Exception as process_sc_e:
                                # Catch errors during the transformation or processing of a single Schedule object
                                logging.error(f"Error processing individual Schedule message (RID={sc_rid}, UID={sc_uid}): {process_sc_e}", exc_info=True)
                                # Continue to the next Schedule object in the list
                     else:
                         logging.debug(f"uR container found in message {sequence_number}, but its 'schedule' list is empty.")

                 # Handle other message types within the uR container if necessary
                 elif getattr(update_messages_container, 'schedule', None): # Example check for schedule
                     logging.debug(f"Found schedule update in uR (seq={sequence_number}). Ignoring.")
                 elif getattr(update_messages_container, 'association', None): # Example check for association
                      logging.debug(f"Found association update in uR (seq={sequence_number}). Ignoring.")
                 # Add checks for OW, trainAlert, trainOrder, etc. if needed
                 else:
                      logging.debug(f"uR container in message {sequence_number} did not contain TS or other handled types.")

            # 4. Handle Snapshot Response (<sR> container)
            elif getattr(pushport_obj, 'sR', None):
                logging.info(f"Received Snapshot Response (sR) message sequence={sequence_number}.")
                snapshot_container = getattr(pushport_obj, 'sR')
                # Check for TS in snapshot
                ts_list_obj_sr = getattr(snapshot_container, 'TS', None)
                # Check for Schedule in snapshot
                schedule_list_obj_sr = getattr(snapshot_container, 'schedule', None)

                if ts_list_obj_sr:
                    # --- [Existing TS snapshot processing code remains here] ---
                    if len(ts_list_obj_sr) > 0:
                        logging.info(f"Found {len(ts_list_obj_sr)} Train Status (TS) updates in sR message sequence={sequence_number}.")
                        # ... (rest of your TS snapshot processing loop) ...
                        for train_status_obj in ts_list_obj_sr:
                             # ...
                             try:
                                 transformed_station_data_list = transform_ts_message(train_status_obj)
                                 if transformed_station_data_list:
                                     # ... logging/writing TS snapshot data ...
                                      try:
                                           with open("forecast_updates_snapshot.log", "a") as f:
                                               f.write(json.dumps(station_data) + "\n")
                                           logging.debug(f"Logged snapshot forecast update for RID {station_data.get('rid')} TPL {station_data.get('tpl')} to forecast_updates_snapshot.log")
                                      except IOError as e:
                                           logging.error(f"Could not write to forecast_updates_snapshot.log: {e}")
                                 # ...
                             except Exception as process_ts_sr_e:
                                  logging.error(f"Error processing individual TS message from sR (RID={getattr(train_status_obj, 'rid', '???')}): {process_ts_sr_e}", exc_info=True)

                # *** NEW: Handle Schedule in snapshot ***
                elif schedule_list_obj_sr:
                    if len(schedule_list_obj_sr) > 0:
                        logging.info(f"Found {len(schedule_list_obj_sr)} Schedule (schedule) updates in sR message sequence={sequence_number}.")
                        for schedule_obj in schedule_list_obj_sr:
                            sc_rid = getattr(schedule_obj, 'rid', 'UNKNOWN_RID')
                            sc_uid = getattr(schedule_obj, 'uid', 'UNKNOWN_UID')
                            try:
                                transformed_schedule_locs = transform_schedule_message(schedule_obj)
                                if transformed_schedule_locs:
                                    logging.debug(f"Transformed {len(transformed_schedule_locs)} locations for Schedule RID={sc_rid} from sR.")
                                    for schedule_loc_data in transformed_schedule_locs:
                                        schedule_loc_data['processing_ts'] = received_time
                                        schedule_loc_data['pp_sequence'] = push_port_sequence
                                        logging.debug(f"Transformed Schedule Loc Update from sR: {json.dumps(schedule_loc_data)}")
                                        json_payload = json.dumps(schedule_loc_data).encode('utf-8')
                                        try:
                                            # Log snapshot schedules separately?
                                            with open("schedule_updates_snapshot.log", "a") as f:
                                                f.write(json.dumps(schedule_loc_data) + "\n")
                                            logging.debug(f"Logged snapshot schedule update for RID {sc_rid} TPL {schedule_loc_data.get('tpl')} to schedule_updates_snapshot.log")
                                        except IOError as e:
                                            logging.error(f"Could not write to schedule_updates_snapshot.log: {e}")
                                        # Send snapshot schedules to Kafka if needed
                                else:
                                    logging.warning(f"Schedule message (from sR) transformation completed for RID {sc_rid} but no valid locations generated.")
                            except Exception as process_sc_sr_e:
                                logging.error(f"Error processing individual Schedule message from sR (RID={sc_rid}, UID={sc_uid}): {process_sc_sr_e}", exc_info=True)
                    else:
                         logging.debug(f"sR container found in message {sequence_number}, but its 'schedule' list is empty.")
                else:
                     logging.debug(f"sR container in message {sequence_number} did not contain TS or schedule elements.")

            # --- [Rest of the on_message logic: FailureResp, unhandled types, general exception] ---
            # ... remains the same ...

        except Exception as general_e:
            logging.error(f"General error processing message sequence={sequence_number}, type={message_type}: {general_e}", exc_info=True)

if __name__ == "__main__":
    conn = None
    while True:
        try:
            logging.info("Attempting to establish STOMP connection...")
            # Use Connection12 explicitly if required by broker, otherwise Connection defaults usually work
            conn = stomp.Connection12([(HOSTNAME, HOSTPORT)],
                                      auto_decode=False, # We handle decoding after decompression
                                      heartbeats=(HEARTBEAT_INTERVAL_MS, HEARTBEAT_INTERVAL_MS))

            # Pass the connection object to the listener upon creation
            listener = StompClient(conn)
            conn.set_listener('', listener)

            connect_and_subscribe(conn) # Handles connect and subscribe logic

            logging.info("Client connected and subscribed. Listening for messages...")

            # Keep the main thread alive while the listener handles messages in its own thread (stomp.py internals)
            while conn.is_connected():
                time.sleep(1) # Check connection status periodically

            # If loop exits, it means is_connected() returned False (likely due to on_disconnected or on_heartbeat_timeout)
            logging.warning(f"Connection lost detected by main loop. Attempting to reconnect in {RECONNECT_DELAY_SECS} seconds...")
            # Clean disconnect just in case listener didn't fully disconnect
            try:
                if conn and not conn.transport.is_connected(): # Check transport layer specifically
                     pass # Already disconnected
                elif conn:
                     conn.disconnect()
            except Exception as disc_e:
                 logging.warning(f"Exception during cleanup disconnect: {disc_e}")
            conn = None # Ensure we create a new connection object next iteration
            time.sleep(RECONNECT_DELAY_SECS)

        except (stomp.exception.ConnectFailedException, stomp.exception.NotConnectedException) as stomp_e:
             logging.error(f"STOMP Connection Error: {stomp_e}. Retrying in {RECONNECT_DELAY_SECS} seconds...")
             if conn:
                 try:
                     conn.disconnect()
                 except Exception as disc_e:
                     logging.warning(f"Exception during disconnect after STOMP error: {disc_e}")
             conn = None
             time.sleep(RECONNECT_DELAY_SECS)
        except KeyboardInterrupt:
             logging.info("KeyboardInterrupt received. Shutting down...")
             break # Exit the main loop
        except Exception as e:
            # Catch other potential errors during connection setup/main loop
            logging.error(f"Unexpected error in main loop: {e}", exc_info=True)
            if conn and conn.is_connected():
                 try:
                     conn.disconnect()
                 except Exception as disc_e:
                     logging.error(f"Error during disconnect after unexpected error: {disc_e}")
            conn = None
            logging.warning(f"Retrying connection in {RECONNECT_DELAY_SECS} seconds...")
            time.sleep(RECONNECT_DELAY_SECS)
        # No finally block needed here as the loop handles cleanup/retry

    # Graceful shutdown
    logging.info("Client shutdown sequence initiated.")
    if conn and conn.is_connected():
        try:
            logging.info("Disconnecting active STOMP connection...")
            conn.disconnect()
            logging.info("STOMP connection disconnected.")
        except Exception as disc_e:
             logging.error(f"Error during final disconnect: {disc_e}")

    # Kafka producer shutdown (uncomment if using Kafka)
    # logging.info("Closing Kafka producer...")
    # if 'producer' in locals() and producer:
    #     producer.flush() # Ensure all messages are sent
    #     producer.close()
    #     logging.info("Kafka producer closed.")

    logging.info("Client has shut down.")